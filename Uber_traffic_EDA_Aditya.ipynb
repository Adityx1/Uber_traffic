{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Traffic Data Cleaning & Feature Engineering"
      ],
      "metadata": {
        "id": "8KCn6r0mJAgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "CgQoy2k7tMYU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD RAW DATA\n",
        "\n",
        "# Load your CSV file (adjust the filename as needed)\n",
        "df = pd.read_csv('Dataset_Uber Traffic.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lru9dIAtQyA",
        "outputId": "302c46ce-3770-411d-8af5-14011a1397c6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (48120, 4)\n",
            "Columns: ['DateTime', 'Junction', 'Vehicles', 'ID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkeSAUrJ65ex",
        "outputId": "b54249a8-1028-4734-b21d-3d330f29027d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few rows:\n",
            "        DateTime  Junction  Vehicles           ID\n",
            "0  01/11/15 0:00         1        15  20151101001\n",
            "1  01/11/15 1:00         1        13  20151101011\n",
            "2  01/11/15 2:00         1        10  20151101021\n",
            "3  01/11/15 3:00         1         7  20151101031\n",
            "4  01/11/15 4:00         1         9  20151101041\n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48120 entries, 0 to 48119\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   DateTime  48120 non-null  object\n",
            " 1   Junction  48120 non-null  int64 \n",
            " 2   Vehicles  48120 non-null  int64 \n",
            " 3   ID        48120 non-null  int64 \n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 1.5+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA CLEANING\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_summary = missing_values[missing_values > 0]\n",
        "if len(missing_summary) > 0:\n",
        "    print(missing_summary)\n",
        "else:\n",
        "    print(\"No missing values found!\")\n",
        "print()\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Data cleaning operations\n",
        "print(\"Performing data cleaning operations...\")\n",
        "\n",
        "# Handle missing values\n",
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Strategy 1: Fill numerical columns with median\n",
        "for col in numerical_columns:\n",
        "    missing_count = df[col].isnull().sum()\n",
        "    if missing_count > 0:\n",
        "        median_value = df[col].median()\n",
        "        df[col].fillna(median_value, inplace=True)\n",
        "        print(f\"Filled {missing_count} missing values in '{col}' with median ({median_value:.2f})\")\n",
        "\n",
        "# Strategy 2: Fill categorical columns with mode\n",
        "for col in categorical_columns:\n",
        "    missing_count = df[col].isnull().sum()\n",
        "    if missing_count > 0:\n",
        "        mode_value = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
        "        df[col].fillna(mode_value, inplace=True)\n",
        "        print(f\"Filled {missing_count} missing values in '{col}' with mode ('{mode_value}')\")\n",
        "\n",
        "# Remove duplicates\n",
        "original_shape = df.shape[0]\n",
        "df = df.drop_duplicates()\n",
        "removed_duplicates = original_shape - df.shape[0]\n",
        "print(f\"Removed {removed_duplicates} duplicate rows\")\n",
        "\n",
        "# Correct data types\n",
        "datetime_columns = ['datetime', 'timestamp', 'date', 'time']\n",
        "existing_datetime_cols = [col for col in datetime_columns if col in df.columns]\n",
        "\n",
        "for col in existing_datetime_cols:\n",
        "    try:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "        print(f\"Converted '{col}' to datetime\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not convert '{col}' to datetime: {e}\")\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHrGMUAN7KBD",
        "outputId": "727d4cdd-e317-4952-e05a-47a246d98d36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            "No missing values found!\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "Performing data cleaning operations...\n",
            "Removed 0 duplicate rows\n",
            "Dataset shape after cleaning: (48120, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AGGREGATE TRAFFIC DATA\n",
        "\n",
        "# Auto-detect or create datetime column\n",
        "if 'datetime' not in df.columns:\n",
        "    if 'date' in df.columns and 'time' in df.columns:\n",
        "        print(\"Creating datetime from separate date and time columns...\")\n",
        "        df['datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'].astype(str))\n",
        "    else:\n",
        "        print(\"Creating sample datetime column for demonstration...\")\n",
        "        df['datetime'] = pd.date_range(start='2023-01-01', periods=len(df), freq='H')\n",
        "\n",
        "# Auto-detect key columns\n",
        "junction_col = None\n",
        "vehicle_count_col = None\n",
        "\n",
        "# Try to find junction column\n",
        "junction_candidates = ['junction_id', 'junction', 'location_id', 'location', 'site_id', 'site']\n",
        "for candidate in junction_candidates:\n",
        "    if candidate in df.columns:\n",
        "        junction_col = candidate\n",
        "        break\n",
        "\n",
        "# Try to find traffic/vehicle count column\n",
        "count_candidates = ['vehicle_count', 'traffic_count', 'count', 'vehicles', 'traffic_volume', 'volume']\n",
        "for candidate in count_candidates:\n",
        "    if candidate in df.columns:\n",
        "        vehicle_count_col = candidate\n",
        "        break\n",
        "\n",
        "# If not found, use the first numerical column\n",
        "if vehicle_count_col is None:\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numerical_cols) > 0:\n",
        "        vehicle_count_col = numerical_cols[0]\n",
        "        print(f\"Using '{vehicle_count_col}' as traffic count column\")\n",
        "\n",
        "# Set datetime as index for aggregation\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "df_temp = df.set_index('datetime')\n",
        "\n",
        "print(\"Aggregating data into hourly intervals...\")\n",
        "print(f\"Junction column: {junction_col}\")\n",
        "print(f\"Traffic count column: {vehicle_count_col}\")\n",
        "\n",
        "# Hourly aggregation\n",
        "if junction_col and junction_col in df_temp.columns:\n",
        "    print(\"Aggregating by junction and hour...\")\n",
        "    df_hourly = df_temp.groupby([pd.Grouper(freq='H'), junction_col]).agg({\n",
        "        vehicle_count_col: ['sum', 'mean', 'max', 'min', 'count']\n",
        "    }).round(2)\n",
        "    df_hourly.columns = ['total_vehicles', 'avg_vehicles', 'max_vehicles', 'min_vehicles', 'observation_count']\n",
        "    df_hourly = df_hourly.reset_index()\n",
        "else:\n",
        "    print(\"Aggregating by hour only...\")\n",
        "    df_hourly = df_temp.groupby(pd.Grouper(freq='H')).agg({\n",
        "        vehicle_count_col: ['sum', 'mean', 'max', 'min', 'count']\n",
        "    }).round(2)\n",
        "    df_hourly.columns = ['total_vehicles', 'avg_vehicles', 'max_vehicles', 'min_vehicles', 'observation_count']\n",
        "    df_hourly = df_hourly.reset_index()\n",
        "\n",
        "print(f\"Hourly aggregated data shape: {df_hourly.shape}\")\n",
        "print(\"Sample of aggregated data:\")\n",
        "print(df_hourly.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxjowowh7U_I",
        "outputId": "249b40fa-2ef5-4953-bbf5-0c6057edf815"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sample datetime column for demonstration...\n",
            "Using 'Junction' as traffic count column\n",
            "Aggregating data into hourly intervals...\n",
            "Junction column: None\n",
            "Traffic count column: Junction\n",
            "Aggregating by hour only...\n",
            "Hourly aggregated data shape: (48120, 6)\n",
            "Sample of aggregated data:\n",
            "             datetime  total_vehicles  avg_vehicles  max_vehicles  min_vehicles  observation_count\n",
            "0 2023-01-01 00:00:00               1           1.0             1             1                  1\n",
            "1 2023-01-01 01:00:00               1           1.0             1             1                  1\n",
            "2 2023-01-01 02:00:00               1           1.0             1             1                  1\n",
            "3 2023-01-01 03:00:00               1           1.0             1             1                  1\n",
            "4 2023-01-01 04:00:00               1           1.0             1             1                  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PREPROCESSING (NORMALIZATION)\n",
        "\n",
        "# Select numerical columns for normalization\n",
        "numerical_cols = df_hourly.select_dtypes(include=[np.number]).columns\n",
        "print(f\"Numerical columns for normalization: {list(numerical_cols)}\")\n",
        "\n",
        "# Create normalized versions using sklearn\n",
        "scaler_standard = StandardScaler()\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "# Standardization (Z-score normalization)\n",
        "df_standardized = df_hourly.copy()\n",
        "df_standardized[numerical_cols] = scaler_standard.fit_transform(df_hourly[numerical_cols])\n",
        "\n",
        "# Min-Max normalization\n",
        "df_normalized = df_hourly.copy()\n",
        "df_normalized[numerical_cols] = scaler_minmax.fit_transform(df_hourly[numerical_cols])\n",
        "\n",
        "print(\"Normalization completed using sklearn scalers!\")\n",
        "print(\"Original data statistics:\")\n",
        "print(df_hourly[numerical_cols].describe())\n",
        "print(\"\\nStandardized data statistics:\")\n",
        "print(df_standardized[numerical_cols].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-uetr8M7fE4",
        "outputId": "7c5cc181-06c1-4c9c-a390-aab69cea35e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical columns for normalization: ['total_vehicles', 'avg_vehicles', 'max_vehicles', 'min_vehicles', 'observation_count']\n",
            "Normalization completed using sklearn scalers!\n",
            "Original data statistics:\n",
            "       total_vehicles  avg_vehicles  max_vehicles  min_vehicles  observation_count\n",
            "count    48120.000000  48120.000000  48120.000000  48120.000000            48120.0\n",
            "mean         2.180549      2.180549      2.180549      2.180549                1.0\n",
            "std          0.966955      0.966955      0.966955      0.966955                0.0\n",
            "min          1.000000      1.000000      1.000000      1.000000                1.0\n",
            "25%          1.000000      1.000000      1.000000      1.000000                1.0\n",
            "50%          2.000000      2.000000      2.000000      2.000000                1.0\n",
            "75%          3.000000      3.000000      3.000000      3.000000                1.0\n",
            "max          4.000000      4.000000      4.000000      4.000000                1.0\n",
            "\n",
            "Standardized data statistics:\n",
            "       total_vehicles  avg_vehicles  max_vehicles  min_vehicles  observation_count\n",
            "count    4.812000e+04  4.812000e+04  4.812000e+04  4.812000e+04            48120.0\n",
            "mean     1.512044e-16  1.512044e-16  1.512044e-16  1.512044e-16                0.0\n",
            "std      1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00                0.0\n",
            "min     -1.220905e+00 -1.220905e+00 -1.220905e+00 -1.220905e+00                0.0\n",
            "25%     -1.220905e+00 -1.220905e+00 -1.220905e+00 -1.220905e+00                0.0\n",
            "50%     -1.867206e-01 -1.867206e-01 -1.867206e-01 -1.867206e-01                0.0\n",
            "75%      8.474640e-01  8.474640e-01  8.474640e-01  8.474640e-01                0.0\n",
            "max      1.881649e+00  1.881649e+00  1.881649e+00  1.881649e+00                0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE ENGINEERING\n",
        "\n",
        "# Work with the original hourly data\n",
        "df_features = df_hourly.copy()\n",
        "\n",
        "# Ensure datetime column\n",
        "df_features['datetime'] = pd.to_datetime(df_features['datetime'])\n",
        "\n",
        "print(\"Creating time-based features...\")\n",
        "\n",
        "# Basic time features\n",
        "df_features['hour'] = df_features['datetime'].dt.hour\n",
        "df_features['day_of_week'] = df_features['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "df_features['day_name'] = df_features['datetime'].dt.day_name()\n",
        "df_features['month'] = df_features['datetime'].dt.month\n",
        "df_features['quarter'] = df_features['datetime'].dt.quarter\n",
        "df_features['year'] = df_features['datetime'].dt.year\n",
        "df_features['day_of_month'] = df_features['datetime'].dt.day\n",
        "df_features['week_of_year'] = df_features['datetime'].dt.isocalendar().week\n",
        "\n",
        "# Binary indicators\n",
        "df_features['is_weekend'] = (df_features['day_of_week'] >= 5).astype(int)\n",
        "df_features['is_monday'] = (df_features['day_of_week'] == 0).astype(int)\n",
        "df_features['is_tuesday'] = (df_features['day_of_week'] == 1).astype(int)\n",
        "df_features['is_wednesday'] = (df_features['day_of_week'] == 2).astype(int)\n",
        "df_features['is_thursday'] = (df_features['day_of_week'] == 3).astype(int)\n",
        "df_features['is_friday'] = (df_features['day_of_week'] == 4).astype(int)\n",
        "df_features['is_saturday'] = (df_features['day_of_week'] == 5).astype(int)\n",
        "df_features['is_sunday'] = (df_features['day_of_week'] == 6).astype(int)\n",
        "\n",
        "# Time period categorization\n",
        "def categorize_time_period(hour):\n",
        "    if 6 <= hour <= 9:\n",
        "        return 'morning_rush'\n",
        "    elif 17 <= hour <= 19:\n",
        "        return 'evening_rush'\n",
        "    elif 10 <= hour <= 16:\n",
        "        return 'midday'\n",
        "    elif 20 <= hour <= 23:\n",
        "        return 'evening'\n",
        "    else:\n",
        "        return 'night'\n",
        "\n",
        "df_features['time_period'] = df_features['hour'].apply(categorize_time_period)\n",
        "\n",
        "# Create binary indicators for time periods\n",
        "time_periods = ['morning_rush', 'evening_rush', 'midday', 'evening', 'night']\n",
        "for period in time_periods:\n",
        "    df_features[f'is_{period}'] = (df_features['time_period'] == period).astype(int)\n",
        "\n",
        "print(\"Creating seasonal and cyclical features...\")\n",
        "\n",
        "# Cyclical encoding for circular features\n",
        "def cyclical_encode(values, max_val):\n",
        "    \"\"\"Encode cyclical features using sin/cos\"\"\"\n",
        "    return np.sin(2 * np.pi * values / max_val), np.cos(2 * np.pi * values / max_val)\n",
        "\n",
        "# Cyclical features\n",
        "df_features['hour_sin'], df_features['hour_cos'] = cyclical_encode(df_features['hour'], 24)\n",
        "df_features['day_of_week_sin'], df_features['day_of_week_cos'] = cyclical_encode(df_features['day_of_week'], 7)\n",
        "df_features['month_sin'], df_features['month_cos'] = cyclical_encode(df_features['month'], 12)\n",
        "df_features['day_of_month_sin'], df_features['day_of_month_cos'] = cyclical_encode(df_features['day_of_month'], 31)\n",
        "\n",
        "print(\"Creating lag features...\")\n",
        "\n",
        "# Sort data properly for lag calculation\n",
        "sort_columns = ['datetime']\n",
        "if junction_col and junction_col in df_features.columns:\n",
        "    sort_columns.append(junction_col)\n",
        "\n",
        "df_features = df_features.sort_values(sort_columns)\n",
        "\n",
        "# Define target column\n",
        "target_column = 'total_vehicles'\n",
        "\n",
        "# Create lag features\n",
        "lag_hours = [1, 2, 3, 6, 12, 24, 48, 168]  # Various time lags including 1 week (168 hours)\n",
        "for lag in lag_hours:\n",
        "    col_name = f'{target_column}_lag_{lag}h'\n",
        "    if junction_col and junction_col in df_features.columns:\n",
        "        df_features[col_name] = df_features.groupby(junction_col)[target_column].shift(lag)\n",
        "    else:\n",
        "        df_features[col_name] = df_features[target_column].shift(lag)\n",
        "\n",
        "print(\"Creating rolling window features...\")\n",
        "\n",
        "# Rolling window features\n",
        "windows = [3, 6, 12, 24, 48, 168]  # Including weekly rolling window\n",
        "for window in windows:\n",
        "    if junction_col and junction_col in df_features.columns:\n",
        "        # Rolling mean\n",
        "        df_features[f'{target_column}_rolling_mean_{window}h'] = (\n",
        "            df_features.groupby(junction_col)[target_column]\n",
        "            .rolling(window=window, min_periods=1)\n",
        "            .mean()\n",
        "            .reset_index(level=0, drop=True)\n",
        "        )\n",
        "        # Rolling std\n",
        "        df_features[f'{target_column}_rolling_std_{window}h'] = (\n",
        "            df_features.groupby(junction_col)[target_column]\n",
        "            .rolling(window=window, min_periods=1)\n",
        "            .std()\n",
        "            .reset_index(level=0, drop=True)\n",
        "        )\n",
        "        # Rolling max\n",
        "        df_features[f'{target_column}_rolling_max_{window}h'] = (\n",
        "            df_features.groupby(junction_col)[target_column]\n",
        "            .rolling(window=window, min_periods=1)\n",
        "            .max()\n",
        "            .reset_index(level=0, drop=True)\n",
        "        )\n",
        "        # Rolling min\n",
        "        df_features[f'{target_column}_rolling_min_{window}h'] = (\n",
        "            df_features.groupby(junction_col)[target_column]\n",
        "            .rolling(window=window, min_periods=1)\n",
        "            .min()\n",
        "            .reset_index(level=0, drop=True)\n",
        "        )\n",
        "    else:\n",
        "        df_features[f'{target_column}_rolling_mean_{window}h'] = df_features[target_column].rolling(window=window, min_periods=1).mean()\n",
        "        df_features[f'{target_column}_rolling_std_{window}h'] = df_features[target_column].rolling(window=window, min_periods=1).std()\n",
        "        df_features[f'{target_column}_rolling_max_{window}h'] = df_features[target_column].rolling(window=window, min_periods=1).max()\n",
        "        df_features[f'{target_column}_rolling_min_{window}h'] = df_features[target_column].rolling(window=window, min_periods=1).min()\n",
        "\n",
        "print(\"Creating difference and rate of change features...\")\n",
        "\n",
        "# Difference features (rate of change)\n",
        "if junction_col and junction_col in df_features.columns:\n",
        "    df_features[f'{target_column}_diff_1h'] = df_features.groupby(junction_col)[target_column].diff(1)\n",
        "    df_features[f'{target_column}_diff_24h'] = df_features.groupby(junction_col)[target_column].diff(24)\n",
        "    df_features[f'{target_column}_pct_change_1h'] = df_features.groupby(junction_col)[target_column].pct_change(1)\n",
        "    df_features[f'{target_column}_pct_change_24h'] = df_features.groupby(junction_col)[target_column].pct_change(24)\n",
        "else:\n",
        "    df_features[f'{target_column}_diff_1h'] = df_features[target_column].diff(1)\n",
        "    df_features[f'{target_column}_diff_24h'] = df_features[target_column].diff(24)\n",
        "    df_features[f'{target_column}_pct_change_1h'] = df_features[target_column].pct_change(1)\n",
        "    df_features[f'{target_column}_pct_change_24h'] = df_features[target_column].pct_change(24)\n",
        "\n",
        "print(\"Creating special event indicators...\")\n",
        "\n",
        "# Holiday and special event indicators\n",
        "holidays = ['2023-01-01', '2023-07-04', '2023-11-23', '2023-12-25']  # Customize for your region\n",
        "df_features['is_holiday'] = df_features['datetime'].dt.date.astype(str).isin(holidays).astype(int)\n",
        "\n",
        "# Month patterns\n",
        "df_features['is_month_start'] = (df_features['datetime'].dt.day <= 3).astype(int)\n",
        "df_features['is_month_end'] = (df_features['datetime'].dt.day >= df_features['datetime'].dt.daysinmonth - 2).astype(int)\n",
        "\n",
        "# Season indicators\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'winter'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'spring'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'summer'\n",
        "    else:\n",
        "        return 'autumn'\n",
        "\n",
        "df_features['season'] = df_features['month'].apply(get_season)\n",
        "\n",
        "# Create binary indicators for seasons\n",
        "seasons = ['winter', 'spring', 'summer', 'autumn']\n",
        "for season in seasons:\n",
        "    df_features[f'is_{season}'] = (df_features['season'] == season).astype(int)\n",
        "\n",
        "print(\"Creating interaction features...\")\n",
        "\n",
        "# Interaction features\n",
        "df_features['hour_weekend_interaction'] = df_features['hour'] * df_features['is_weekend']\n",
        "df_features['is_rush_hour'] = ((df_features['is_morning_rush'] == 1) | (df_features['is_evening_rush'] == 1)).astype(int)\n",
        "df_features['rush_hour_weekend'] = df_features['is_rush_hour'] * df_features['is_weekend']\n",
        "\n",
        "print(f\"Feature engineering completed! Dataset shape: {df_features.shape}\")\n",
        "print(f\"Features created: {len(df_features.columns) - len(df_hourly.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxNxOgSp7ouR",
        "outputId": "7ee1a51a-a906-454a-a0af-4ecabc4d36d1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating time-based features...\n",
            "Creating seasonal and cyclical features...\n",
            "Creating lag features...\n",
            "Creating rolling window features...\n",
            "Creating difference and rate of change features...\n",
            "Creating special event indicators...\n",
            "Creating interaction features...\n",
            "Feature engineering completed! Dataset shape: (48120, 83)\n",
            "Features created: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE IMPORTANCE AND SELECTION\n",
        "\n",
        "# Prepare data for feature importance analysis\n",
        "feature_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove the target column from features if it exists\n",
        "if target_column in feature_cols:\n",
        "    feature_cols.remove(target_column)\n",
        "\n",
        "# Remove highly correlated lag features that might cause data leakage for analysis\n",
        "original_feature_cols = feature_cols.copy()\n",
        "\n",
        "# Create feature matrix (keeping lag features for now)\n",
        "X = df_features[feature_cols].fillna(0)\n",
        "y = df_features[target_column].fillna(df_features[target_column].median())\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target variable shape: {y.shape}\")\n",
        "\n",
        "# Remove rows where target is still NaN\n",
        "valid_idx = ~y.isna()\n",
        "X = X[valid_idx]\n",
        "y = y[valid_idx]\n",
        "\n",
        "print(f\"After removing NaN targets: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "if len(X) > 0 and len(y) > 0:\n",
        "    # Correlation analysis\n",
        "    print(\"\\nCorrelation Analysis:\")\n",
        "    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
        "    print(\"Top 15 features by correlation with target:\")\n",
        "    print(correlations.head(15))\n",
        "\n",
        "    # Random Forest Feature Importance (lightweight analysis)\n",
        "    print(\"\\nRandom Forest Feature Importance Analysis:\")\n",
        "    try:\n",
        "        rf = RandomForestRegressor(\n",
        "            n_estimators=50, random_state=42, n_jobs=-1, max_depth=10\n",
        "        )\n",
        "        rf.fit(X, y)\n",
        "\n",
        "        feature_importance = pd.DataFrame(\n",
        "            {\"feature\": X.columns, \"importance\": rf.feature_importances_}\n",
        "        ).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "        print(\"Top 20 features by Random Forest importance:\")\n",
        "        print(feature_importance.head(20))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Random Forest analysis failed: {e}\")\n",
        "        feature_importance = pd.DataFrame(\n",
        "            {\"feature\": X.columns, \"importance\": [0] * len(X.columns)}\n",
        "        )\n",
        "\n",
        "    # Statistical feature selection using F-test\n",
        "    print(\"\\nStatistical Feature Selection (F-test):\")\n",
        "    try:\n",
        "        k_best = SelectKBest(score_func=f_regression, k=min(20, len(feature_cols)))\n",
        "        X_selected = k_best.fit_transform(X, y)\n",
        "\n",
        "        selected_features = X.columns[k_best.get_support()]\n",
        "        feature_scores = pd.DataFrame(\n",
        "            {\n",
        "                \"feature\": X.columns,\n",
        "                \"f_score\": k_best.scores_,\n",
        "                \"selected\": k_best.get_support(),\n",
        "            }\n",
        "        ).sort_values(\"f_score\", ascending=False)\n",
        "\n",
        "        print(\"Top 20 features by F-score:\")\n",
        "        print(feature_scores.head(20))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"F-test analysis failed: {e}\")\n",
        "        feature_scores = pd.DataFrame(\n",
        "            {\n",
        "                \"feature\": X.columns,\n",
        "                \"f_score\": [0] * len(X.columns),\n",
        "                \"selected\": [False] * len(X.columns),\n",
        "            }\n",
        "        )\n",
        "        selected_features = X.columns[:20]\n",
        "\n",
        "    # Create final optimized feature set\n",
        "    print(\"\\nCreating final optimized feature set...\")\n",
        "\n",
        "    # Combine top features from different methods\n",
        "    top_corr_features = correlations.head(15).index.tolist()\n",
        "    top_rf_features = feature_importance.head(15)[\"feature\"].tolist()\n",
        "    top_stat_features = selected_features.tolist()\n",
        "\n",
        "    # Combine and deduplicate\n",
        "    final_features = list(set(top_corr_features + top_rf_features + top_stat_features))\n",
        "\n",
        "    print(f\"Final optimized feature set contains {len(final_features)} features:\")\n",
        "    for i, feature in enumerate(final_features, 1):\n",
        "        print(f\"{i:2d}. {feature}\")\n",
        "\n",
        "    # Create final dataset\n",
        "    essential_columns = [\"datetime\"]\n",
        "    if junction_col and junction_col in df_features.columns:\n",
        "        essential_columns.append(junction_col)\n",
        "    essential_columns.append(target_column)\n",
        "\n",
        "    df_final = df_features[essential_columns + final_features].copy()\n",
        "    print(f\"\\nFinal optimized dataset shape: {df_final.shape}\")\n",
        "    print(\"\\nFinal dataset sample:\")\n",
        "    print(df_final.head())\n",
        "\n",
        "else:\n",
        "    print(\"Insufficient data for feature importance analysis\")\n",
        "    df_final = df_features.copy()\n",
        "    final_features = feature_cols[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QmlJvod72It",
        "outputId": "84cda279-b88f-44f0-bc83-f888cd1d2aa8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (48120, 78)\n",
            "Target variable shape: (48120,)\n",
            "After removing NaN targets: X=(48120, 78), y=(48120,)\n",
            "\n",
            "Correlation Analysis:\n",
            "Top 15 features by correlation with target:\n",
            "avg_vehicles                       1.000000\n",
            "max_vehicles                       1.000000\n",
            "min_vehicles                       1.000000\n",
            "total_vehicles_rolling_max_48h     1.000000\n",
            "total_vehicles_rolling_max_3h      1.000000\n",
            "total_vehicles_rolling_max_168h    1.000000\n",
            "total_vehicles_rolling_max_12h     1.000000\n",
            "total_vehicles_rolling_max_6h      1.000000\n",
            "total_vehicles_rolling_max_24h     1.000000\n",
            "total_vehicles_rolling_mean_3h     0.999981\n",
            "total_vehicles_lag_1h              0.999956\n",
            "total_vehicles_rolling_mean_6h     0.999949\n",
            "total_vehicles_rolling_min_3h      0.999933\n",
            "total_vehicles_lag_2h              0.999911\n",
            "total_vehicles_rolling_mean_12h    0.999883\n",
            "dtype: float64\n",
            "\n",
            "Random Forest Feature Importance Analysis:\n",
            "Top 20 features by Random Forest importance:\n",
            "                             feature  importance\n",
            "58    total_vehicles_rolling_max_48h    0.112495\n",
            "56   total_vehicles_rolling_mean_48h    0.102620\n",
            "50    total_vehicles_rolling_max_12h    0.079654\n",
            "54    total_vehicles_rolling_max_24h    0.078184\n",
            "46     total_vehicles_rolling_max_6h    0.077049\n",
            "1                       max_vehicles    0.070286\n",
            "60  total_vehicles_rolling_mean_168h    0.069368\n",
            "44    total_vehicles_rolling_mean_6h    0.069171\n",
            "62   total_vehicles_rolling_max_168h    0.055279\n",
            "0                       avg_vehicles    0.049404\n",
            "32             total_vehicles_lag_1h    0.044788\n",
            "42     total_vehicles_rolling_max_3h    0.040045\n",
            "48   total_vehicles_rolling_mean_12h    0.039985\n",
            "52   total_vehicles_rolling_mean_24h    0.038311\n",
            "40    total_vehicles_rolling_mean_3h    0.033003\n",
            "2                       min_vehicles    0.032431\n",
            "33             total_vehicles_lag_2h    0.003240\n",
            "47     total_vehicles_rolling_min_6h    0.003228\n",
            "43     total_vehicles_rolling_min_3h    0.001461\n",
            "13                        is_tuesday    0.000000\n",
            "\n",
            "Statistical Feature Selection (F-test):\n",
            "Top 20 features by F-score:\n",
            "                            feature       f_score  selected\n",
            "0                      avg_vehicles  1.917736e+18      True\n",
            "1                      max_vehicles  1.917736e+18      True\n",
            "2                      min_vehicles  1.917736e+18      True\n",
            "50   total_vehicles_rolling_max_12h  1.917736e+18      True\n",
            "58   total_vehicles_rolling_max_48h  1.917736e+18      True\n",
            "42    total_vehicles_rolling_max_3h  1.917736e+18      True\n",
            "62  total_vehicles_rolling_max_168h  1.917736e+18      True\n",
            "54   total_vehicles_rolling_max_24h  1.917736e+18      True\n",
            "46    total_vehicles_rolling_max_6h  1.917736e+18      True\n",
            "40   total_vehicles_rolling_mean_3h  1.299046e+09      True\n",
            "32            total_vehicles_lag_1h  5.412431e+08      True\n",
            "44   total_vehicles_rolling_mean_6h  4.724189e+08      True\n",
            "43    total_vehicles_rolling_min_3h  3.608144e+08      True\n",
            "33            total_vehicles_lag_2h  2.706311e+08      True\n",
            "48  total_vehicles_rolling_mean_12h  2.054316e+08      True\n",
            "34            total_vehicles_lag_3h  1.804272e+08      True\n",
            "47    total_vehicles_rolling_min_6h  1.443248e+08      True\n",
            "52  total_vehicles_rolling_mean_24h  9.618934e+07      True\n",
            "35            total_vehicles_lag_6h  9.022317e+07      True\n",
            "51   total_vehicles_rolling_min_12h  6.560137e+07      True\n",
            "\n",
            "Creating final optimized feature set...\n",
            "Final optimized feature set contains 22 features:\n",
            " 1. total_vehicles_lag_3h\n",
            " 2. total_vehicles_rolling_mean_24h\n",
            " 3. min_vehicles\n",
            " 4. total_vehicles_rolling_min_12h\n",
            " 5. total_vehicles_rolling_min_3h\n",
            " 6. total_vehicles_rolling_mean_12h\n",
            " 7. total_vehicles_rolling_mean_168h\n",
            " 8. total_vehicles_lag_1h\n",
            " 9. total_vehicles_rolling_max_12h\n",
            "10. total_vehicles_rolling_max_6h\n",
            "11. total_vehicles_rolling_max_168h\n",
            "12. total_vehicles_rolling_mean_3h\n",
            "13. total_vehicles_lag_2h\n",
            "14. avg_vehicles\n",
            "15. total_vehicles_rolling_mean_48h\n",
            "16. total_vehicles_rolling_min_6h\n",
            "17. total_vehicles_rolling_max_3h\n",
            "18. total_vehicles_lag_6h\n",
            "19. max_vehicles\n",
            "20. total_vehicles_rolling_max_48h\n",
            "21. total_vehicles_rolling_mean_6h\n",
            "22. total_vehicles_rolling_max_24h\n",
            "\n",
            "Final optimized dataset shape: (48120, 24)\n",
            "\n",
            "Final dataset sample:\n",
            "             datetime  total_vehicles  total_vehicles_lag_3h  total_vehicles_rolling_mean_24h  min_vehicles  total_vehicles_rolling_min_12h  total_vehicles_rolling_min_3h  total_vehicles_rolling_mean_12h  total_vehicles_rolling_mean_168h  total_vehicles_lag_1h  total_vehicles_rolling_max_12h  total_vehicles_rolling_max_6h  total_vehicles_rolling_max_168h  total_vehicles_rolling_mean_3h  total_vehicles_lag_2h  avg_vehicles  total_vehicles_rolling_mean_48h  total_vehicles_rolling_min_6h  total_vehicles_rolling_max_3h  total_vehicles_lag_6h  max_vehicles  total_vehicles_rolling_max_48h  total_vehicles_rolling_mean_6h  total_vehicles_rolling_max_24h\n",
            "0 2023-01-01 00:00:00               1                    NaN                              1.0             1                             1.0                            1.0                              1.0                               1.0                    NaN                             1.0                            1.0                              1.0                             1.0                    NaN           1.0                              1.0                            1.0                            1.0                    NaN             1                             1.0                             1.0                             1.0\n",
            "1 2023-01-01 01:00:00               1                    NaN                              1.0             1                             1.0                            1.0                              1.0                               1.0                    1.0                             1.0                            1.0                              1.0                             1.0                    NaN           1.0                              1.0                            1.0                            1.0                    NaN             1                             1.0                             1.0                             1.0\n",
            "2 2023-01-01 02:00:00               1                    NaN                              1.0             1                             1.0                            1.0                              1.0                               1.0                    1.0                             1.0                            1.0                              1.0                             1.0                    1.0           1.0                              1.0                            1.0                            1.0                    NaN             1                             1.0                             1.0                             1.0\n",
            "3 2023-01-01 03:00:00               1                    1.0                              1.0             1                             1.0                            1.0                              1.0                               1.0                    1.0                             1.0                            1.0                              1.0                             1.0                    1.0           1.0                              1.0                            1.0                            1.0                    NaN             1                             1.0                             1.0                             1.0\n",
            "4 2023-01-01 04:00:00               1                    1.0                              1.0             1                             1.0                            1.0                              1.0                               1.0                    1.0                             1.0                            1.0                              1.0                             1.0                    1.0           1.0                              1.0                            1.0                            1.0                    NaN             1                             1.0                             1.0                             1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
        "print(f\"Hourly aggregated shape: {df_hourly.shape}\")\n",
        "print(f\"Full feature engineered shape: {df_features.shape}\")\n",
        "print(f\"Final optimized dataset shape: {df_final.shape}\")\n",
        "print(f\"Total features engineered: {len(df_features.columns) - len(df_hourly.columns)}\")\n",
        "print(f\"Features in final optimized set: {len(final_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfX_dUVu9Nqj",
        "outputId": "7f5c0253-6792-4167-a906-fb0aaaf0d0ae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original dataset shape: (48120, 5)\n",
            "Hourly aggregated shape: (48120, 6)\n",
            "Full feature engineered shape: (48120, 83)\n",
            "Final optimized dataset shape: (48120, 24)\n",
            "Total features engineered: 77\n",
            "Features in final optimized set: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL SUMMARY\n",
        "\n",
        "print(\"FINAL PROCESSING SUMMARY\")\n",
        "\n",
        "print(f\"âœ… COMPLETED OPERATIONS:\")\n",
        "print(f\"   â€¢ Data loading and quality assessment\")\n",
        "print(f\"   â€¢ Missing value imputation ({missing_summary.sum()} values handled)\" if len(missing_summary) > 0 else f\"   â€¢ No missing values found\")\n",
        "print(f\"   â€¢ Duplicate removal ({removed_duplicates} rows removed)\")\n",
        "print(f\"   â€¢ Data type corrections and datetime parsing\")\n",
        "print(f\"   â€¢ Hourly traffic aggregation by junction\")\n",
        "print(f\"   â€¢ Standardization and normalization using sklearn\")\n",
        "print(f\"   â€¢ Comprehensive time-based feature engineering\")\n",
        "print(f\"   â€¢ Lag and rolling window feature creation\")\n",
        "print(f\"   â€¢ Cyclical encoding for temporal features\")\n",
        "print(f\"   â€¢ Special event and interaction features\")\n",
        "print(f\"   â€¢ Multi-method feature importance analysis\")\n",
        "print(f\"   â€¢ Feature selection and optimization\")\n",
        "print(f\"   â€¢ Multiple export formats for different use cases\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMhXyd8Fr9eE",
        "outputId": "bfe524f0-1ade-4e3f-8cb0-118e8c8f4f36"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL PROCESSING SUMMARY\n",
            "âœ… COMPLETED OPERATIONS:\n",
            "   â€¢ Data loading and quality assessment\n",
            "   â€¢ No missing values found\n",
            "   â€¢ Duplicate removal (0 rows removed)\n",
            "   â€¢ Data type corrections and datetime parsing\n",
            "   â€¢ Hourly traffic aggregation by junction\n",
            "   â€¢ Standardization and normalization using sklearn\n",
            "   â€¢ Comprehensive time-based feature engineering\n",
            "   â€¢ Lag and rolling window feature creation\n",
            "   â€¢ Cyclical encoding for temporal features\n",
            "   â€¢ Special event and interaction features\n",
            "   â€¢ Multi-method feature importance analysis\n",
            "   â€¢ Feature selection and optimization\n",
            "   â€¢ Multiple export formats for different use cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š FEATURE CATEGORIES CREATED:\n",
        "\n",
        "Time-based features: hour, day, month, season\n",
        "\n",
        "Binary indicators: weekend, weekdays, time periods\n",
        "\n",
        "Cyclical encodings: sin/cos transformations\n",
        "\n",
        "Lag features: 1h to 1-week historical data\n",
        "\n",
        "Rolling statistics: mean, std, max, min windows\n",
        "\n",
        "Rate of change: differences and percentage changes\n",
        "\n",
        "Special events: holidays and month boundaries\n",
        "\n",
        "Interaction features: combined categorical effects\n",
        "\n",
        "ðŸ”§ FEATURE SELECTION METHODS APPLIED:\n",
        "\n",
        "Correlation analysis with target variable\n",
        "\n",
        "Random Forest feature importance ranking\n",
        "\n",
        "Statistical F-test feature selection\n",
        "\n",
        "Combined optimization across methods"
      ],
      "metadata": {
        "id": "ALnFxc6e-GEX"
      }
    }
  ]
}